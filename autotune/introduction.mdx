---
title: "Introduction"
description: "Run evaluations on models and prompts to find the best variants for your agents"
---

Autotune is a different approach to the traditional evals experience. Instead of setting up complex eval pipelines, we simply ingest your production traces and let you 
replay them with different models and generate optimized prompts based on your feedback.

Some of the key features include:

- **Content-based versioning**: Each unique prompt content gets its own version via SHA-256 hashing
- **Variable templating**: Use `{{variable}}` syntax for dynamic content
- **Automatic tracking**: All interactions are traced for analysis
- **One-click model deployments**: Models update instantly without code changes

## How it works

<Steps>
  <Step title="Instrument your code">
    Replace hardcoded prompts with `ze.prompt()` calls
  </Step>
  <Step title="Every change creates a version">
    Each time you modify your prompt content, a new version is automatically created and tracked
  </Step>
  <Step title="Collect performance data">
    ZeroEval automatically tracks all LLM interactions and their outcomes
  </Step>
  <Step title="Tune and evaluate">
    Use the UI to run experiments, vote on outputs, and identify the best prompt/model combinations
  </Step>
  <Step title="One-click model deployments">
    Winning configurations are automatically deployed to your application without code changes
  </Step>
</Steps>

<CardGroup cols={2}>
  <Card title="Setup Guide" icon="wrench" href="/autotune/setup">
    Learn how to integrate ze.prompt() into your codebase
  </Card>
  <Card title="Prompts Guide" icon="sliders" href="/autotune/prompts">
    Run experiments and deploy winning combinations
  </Card>
</CardGroup>

