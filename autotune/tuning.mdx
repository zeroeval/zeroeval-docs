---
title: "Tuning"
description: "Running autotune experiments"
---

# Tuning

With ZeroEval's autotune, you can run experiments to find the best prompt/model combinations and deploy them instantly to your application.

<video
  autoPlay
  muted
  loop
  playsInline
  className="w-full aspect-video rounded-xl"
  src="/videos/humapreference.mp4"
></video>

## The Tuning Workflow

### 1. Create Prompt Versions

As you iterate on your prompts in code, each change automatically creates a new version:

```python
# Each content change creates a new version with a unique hash
v1 = ze.prompt(name="writer", content="Write a blog post about {{topic}}")
v2 = ze.prompt(name="writer", content="Write an engaging blog post about {{topic}}")
v3 = ze.prompt(name="writer", content="Write an SEO-optimized blog post about {{topic}}")
```

### 2. Launch Tuning Experiments

In the ZeroEval UI, you can:

1. **Select a task** to tune (e.g., "writer")
2. **Choose prompt versions** to compare
3. **Select models** to test (GPT-4, Claude, etc.)
4. **Configure experiment parameters**

The system creates all combinations of prompts × models for testing.

### 3. Evaluate with Human Feedback

<Tabs>
  <Tab title="Side-by-Side Comparison">
    View outputs from different prompt/model combinations side-by-side. Vote for the best responses to train the ranking algorithm.
  </Tab>
  <Tab title="Replay Testing">
    Replay any historical conversation with different prompts and models to see how they would have performed.
  </Tab>
  <Tab title="A/B Testing">
    Run live A/B tests where different users get different prompt/model combinations automatically.
  </Tab>
</Tabs>

### 4. Statistical Analysis

- **TrueSkill Ranking**: Advanced algorithm that ranks prompt/model combinations based on voting patterns
- **Confidence Intervals**: See statistical significance after ~20 samples
- **Performance Metrics**: Track latency, cost, and quality scores
- **Winner Detection**: Automatically identifies the best-performing combination

### 5. Deploy Over-the-Air

Once you've identified a winner, deployment is instant:

```python
# In your application, nothing changes!
prompt = ze.prompt(
    name="writer",
    from_="latest"  # Automatically gets the winning version
)

# The model is automatically patched to the winner
response = client.chat.completions.create(
    model="gpt-4",  # ← Replaced with winning model!
    messages=[{"role": "system", "content": prompt}]
)
```

## How Model Patching Works

<Note>
The beauty of ZeroEval's autotune is that your application code doesn't change. Models and prompts update automatically based on your tuning results.
</Note>

Here's what happens under the hood:

1. **Metadata Decoration**: `ze.prompt()` returns content wrapped with metadata:
   ```
   <zeroeval>{"task": "writer", "prompt_version_id": "abc123", ...}</zeroeval>
   Your actual prompt content here...
   ```

2. **OpenAI Integration**: ZeroEval patches OpenAI's SDK to:
   - Detect the `<zeroeval>` metadata in prompts
   - Look up the associated model for that prompt version
   - Replace the model parameter with the tuned model

3. **Transparent Operation**: Your code remains clean and simple while benefiting from continuous optimization

## Advanced Features

### Environment-Specific Deployments

Deploy different versions to different environments:

```python
# Production gets the stable, well-tested version
os.environ["ZEROEVAL_ENV"] = "production"
prompt = ze.prompt(name="writer", from_="latest")  # Gets production version

# Staging can test newer versions
os.environ["ZEROEVAL_ENV"] = "staging"
prompt = ze.prompt(name="writer", from_="latest")  # Gets staging version
```

### Gradual Rollouts

Control deployment with tags:

```python
# Tag specific versions
ze.prompts.tag("writer", version=5, tag="stable")
ze.prompts.tag("writer", version=7, tag="canary")

# Fetch by tag
stable_prompt = ze.get_prompt("writer", tag="stable")
canary_prompt = ze.get_prompt("writer", tag="canary")
```

### Performance Monitoring

Track how your tuned prompts perform in production:

- **Response Quality**: User satisfaction metrics
- **Latency**: Response time by model
- **Cost**: Token usage and pricing
- **Error Rates**: Failures and retries

## Best Practices for Tuning

1. **Start with Clear Objectives**
   - Define what "better" means for your use case
   - Set up evaluation criteria before starting

2. **Test Incrementally**
   - Make small, focused changes between versions
   - Test one variable at a time when possible

3. **Gather Sufficient Data**
   - Wait for statistical significance (typically 20+ samples)
   - Test across diverse inputs and edge cases

4. **Monitor Production Performance**
   - Watch metrics after deployment
   - Be ready to rollback if needed

5. **Iterate Continuously**
   - Tuning is an ongoing process
   - Regular experiments lead to continuous improvement

## Example: E-commerce Support Bot

Here's a real-world example of tuning a customer support bot:

```python
# Version 1: Basic prompt
v1 = ze.prompt(
    name="support-bot",
    content="You are a helpful support agent."
)

# Version 2: More specific
v2 = ze.prompt(
    name="support-bot",
    content="""You are a helpful e-commerce support agent.
    Be concise but friendly. Focus on solving issues quickly."""
)

# Version 3: With examples
v3 = ze.prompt(
    name="support-bot",
    content="""You are a helpful e-commerce support agent.
    Be concise but friendly. Focus on solving issues quickly.
    
    Examples of good responses:
    - "I'll help you track that order right away!"
    - "Let me check your refund status for you."
    """
)
```

After running experiments comparing these versions across GPT-4 and Claude models, you might find that Version 3 with Claude performs best. This winning combination automatically deploys to your application without any code changes!

Ready to start tuning? Head to the [ZeroEval Console](https://app.zeroeval.com) to create your first experiment.
