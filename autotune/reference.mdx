---
title: "Reference"
description: "Parameters and configuration for ze.prompt"
---

`ze.prompt` creates or fetches versioned prompts from the Prompt Library and returns decorated content for downstream LLM calls.

<Info>
**TypeScript differences**: In TypeScript, `ze.prompt()` is an async function that returns `Promise<string>`. Parameters use camelCase and are passed as an options object: `ze.prompt({ name: "...", content: "..." })`.
</Info>

## Parameters

| Python | TypeScript | Type | Required | Default | Description |
| --- | --- | --- | --- | --- | --- |
| `name` | `name` | string | yes | — | Task name associated with the prompt in the library |
| `content` | `content` | string | no | `None`/`undefined` | Raw prompt content to ensure/create a version by content |
| `from_` | `from` | string | no | `None`/`undefined` | Either `"latest"`, `"explicit"`, or a 64‑char SHA‑256 hash |
| `variables` | `variables` | dict/object | no | `None`/`undefined` | Template variables to render `{{variable}}` tokens |

Notes:

- In Python, use `from_` (with underscore) as `from` is a reserved keyword. TypeScript uses `from` directly.
- Exactly one of `content` or `from` must be provided (except when using `from: "explicit"` with `content`).
- `from="latest"` fetches the latest version bound to the task; otherwise `from` must be a 64‑char hex SHA‑256 hash.

## Behavior

- **content provided**: Computes a normalized SHA‑256 hash, ensures a prompt version exists for `name`, and returns decorated content.
- **from="latest"**: Fetches the latest version for `name` and returns decorated content.
- **from=**`<hash>`: Fetches by content hash for `name` and returns decorated content.

Decoration adds a compact metadata header used by integrations:

- `task`, `prompt_slug`, `prompt_version`, `prompt_version_id`, `variables`, and (when created by content) `content_hash`.

OpenAI integration: when `prompt_version_id` is present, the SDK will automatically patch the `model` parameter to the model bound to that prompt version.

## Return Value

- **Python**: `str` - Decorated prompt content ready to pass into LLM clients.
- **TypeScript**: `Promise<string>` - Async function returning decorated prompt content.

## Errors

| Python | TypeScript | When |
| --- | --- | --- |
| `ValueError` | `Error` | Both `content` and `from` provided (except explicit), or neither; invalid `from` value |
| `PromptRequestError` | `PromptRequestError` | `from="latest"` but no versions exist for `name` |
| `PromptNotFoundError` | `PromptNotFoundError` | `from` is a hash that does not exist for `name` |

## Examples

<CodeGroup>
```python Python
import zeroeval as ze

# Create/ensure a version by content
system = ze.prompt(
    name="support-triage",
    content="You are a helpful assistant for {{product}}.",
    variables={"product": "Acme"},
)

# Fetch the latest version for this task
system = ze.prompt(name="support-triage", from_="latest")

# Fetch a specific version by content hash
system = ze.prompt(name="support-triage", from_="c6a7...deadbeef...0123")
```
```typescript TypeScript
import * as ze from 'zeroeval';

// Create/ensure a version by content
const system = await ze.prompt({
  name: "support-triage",
  content: "You are a helpful assistant for {{product}}.",
  variables: { product: "Acme" },
});

// Fetch the latest version for this task
const system = await ze.prompt({ name: "support-triage", from: "latest" });

// Fetch a specific version by content hash
const system = await ze.prompt({ name: "support-triage", from: "c6a7...deadbeef...0123" });
```
</CodeGroup>


