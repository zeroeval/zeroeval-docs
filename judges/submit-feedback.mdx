---
title: "Submitting Feedback"
description: "Programmatically submit feedback for judge evaluations via SDK"
---

## Overview

When calibrating judges, you can submit feedback programmatically using the SDK.
This is useful for:

- Bulk feedback submission from automated pipelines
- Integration with custom review workflows
- Syncing feedback from external labeling tools

Your existing `send_feedback` integrations remain valid. Criterion-level feedback is an optional extension for scored judges.

## Important: Using the Correct IDs

Judge evaluations involve two related spans:

| ID | Description |
|---|---|
| **Source Span ID** | The original LLM call that was evaluated |
| **Judge Call Span ID** | The span created when the judge ran its evaluation |

When submitting feedback, always include the `judge_id` parameter to ensure
feedback is correctly associated with the judge evaluation.

## Python SDK

### From the UI (Recommended)

The easiest way to get the correct IDs is from the Judge Evaluation modal:

1. Open a judge evaluation in the dashboard
2. Expand the "SDK Integration" section
3. Click "Copy" to copy the pre-filled Python code
4. Paste and customize the generated code

### Manual Submission

```python
from zeroeval import ZeroEval

client = ZeroEval()

# Submit feedback for a judge evaluation
client.send_feedback(
    prompt_slug="your-judge-task-slug",  # The task/prompt associated with the judge
    completion_id="span-id-here",         # The span ID from the evaluation
    thumbs_up=True,                        # True = correct, False = incorrect
    reason="Optional explanation",
    judge_id="automation-id-here",         # Required for judge feedback
)
```

### Parameters

| Parameter | Type | Required | Description |
|---|---|---|---|
| `prompt_slug` | str | Yes | The task slug associated with the judge |
| `completion_id` | str | Yes | The span ID being evaluated |
| `thumbs_up` | bool | Yes | `True` if judge was correct, `False` if wrong |
| `reason` | str | No | Explanation of the feedback |
| `judge_id` | str | Yes* | The judge automation ID (*required for judge feedback) |
| `expected_score` | float | No | For scored judges: the expected score value |
| `score_direction` | str | No | For scored judges: `"too_high"` or `"too_low"` |
| `criteria_feedback` | dict | No | For scored judges: per-criterion expected score/reason map |

<Note>
  `expected_score` and `score_direction` are only valid for scored judges 
  (judges with `evaluation_type: "scored"`). The API will return a 400 error 
  if these fields are provided for binary judges.
</Note>

### Step 1: Discover Available Criteria (Scored Judges)

Before sending `criteria_feedback`, fetch valid criterion keys for the judge.

```python
from zeroeval import ZeroEval

client = ZeroEval()

criteria = client.get_judge_criteria(
    project_id="your-project-id",
    judge_id="automation-id-here",
)

print(criteria["evaluation_type"])  # "scored" or "binary"
print(criteria["criteria"])         # [{"key": "...", "label": "...", "description": "..."}]
```

```bash
curl -X GET "https://api.zeroeval.com/projects/{project_id}/judges/{judge_id}/criteria" \
  -H "Authorization: Bearer $ZEROEVAL_API_KEY"
```

### Step 2: Score-Based Feedback (General Score)

For judges using scored rubrics (not binary pass/fail), you can provide additional
feedback about the overall expected score:

```python
from zeroeval import ZeroEval

client = ZeroEval()

# Submit feedback for a scored judge evaluation
client.send_feedback(
    prompt_slug="quality-scorer",
    completion_id="span-id-here",
    thumbs_up=False,                       # The judge was incorrect
    judge_id="automation-id-here",
    expected_score=3.5,                    # What the score should have been
    score_direction="too_high",            # The judge scored too high
    reason="Score should have been lower due to grammar issues",
)
```

### Step 3: Score-Based Feedback (Per-Criterion)

For scored judges, you can send corrections for specific criteria:

```python
from zeroeval import ZeroEval

client = ZeroEval()

client.send_feedback(
    prompt_slug="quality-scorer",
    completion_id="span-id-here",
    thumbs_up=False,
    judge_id="automation-id-here",
    reason="Criterion-level score adjustments",
    criteria_feedback={
        "CTA_text": {
            "expected_score": 4.0,
            "reason": "CTA is clear and prominent"
        },
        "CX-004": {
            "expected_score": 1.0,
            "reason": "Required phone number is missing"
        }
    }
)
```

## REST API

### Binary Judge Feedback

```bash
curl -X POST "https://api.zeroeval.com/v1/prompts/{task_slug}/completions/{span_id}/feedback" \
  -H "Authorization: Bearer $ZEROEVAL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "thumbs_up": true,
    "reason": "Judge correctly identified the issue",
    "judge_id": "automation-uuid-here"
  }'
```

### Scored Judge Feedback

For scored judges, include `expected_score` and `score_direction`:

```bash
curl -X POST "https://api.zeroeval.com/v1/prompts/{task_slug}/completions/{span_id}/feedback" \
  -H "Authorization: Bearer $ZEROEVAL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "thumbs_up": false,
    "reason": "Score should have been lower",
    "judge_id": "automation-uuid-here",
    "expected_score": 3.5,
    "score_direction": "too_high"
  }'
```

### Scored Judge Feedback (Criterion-Level)

```bash
curl -X POST "https://api.zeroeval.com/v1/prompts/{task_slug}/completions/{span_id}/feedback" \
  -H "Authorization: Bearer $ZEROEVAL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "thumbs_up": false,
    "judge_id": "automation-uuid-here",
    "reason": "Criterion-level corrections",
    "criteria_feedback": {
      "CTA_text": {
        "expected_score": 4.0,
        "reason": "CTA is clear and visible"
      },
      "CX-004": {
        "expected_score": 1.0,
        "reason": "Phone number is missing"
      }
    }
  }'
```

## Criteria Payload Shape

`criteria_feedback` uses this shape:

```json
{
  "criteria_feedback": {
    "criterion_key": {
      "expected_score": 4.0,
      "reason": "Optional explanation"
    }
  }
}
```

Validation rules:
- `judge_id` is required when sending `criteria_feedback`
- `criteria_feedback` is allowed only for scored judges (`evaluation_type: "scored"`)

## Finding Your IDs

| ID | Where to Find It |
|---|---|
| **Task Slug** | In the judge settings, or the URL when editing the judge's prompt |
| **Span ID** | In the evaluation modal, or via `get_judge_evaluations()` response |
| **Judge ID** | In the URL when viewing a judge (`/judges/{judge_id}`) |

## Bulk Feedback Submission

For submitting feedback on multiple evaluations, you can iterate through evaluations:

```python
from zeroeval import ZeroEval

client = ZeroEval()

# Get evaluations to review
evaluations = client.get_judge_evaluations(
    project_id="your-project-id",
    judge_id="your-judge-id",
    limit=100,
)

# Submit feedback for each
for eval in evaluations["evaluations"]:
    # Your logic to determine if the evaluation was correct
    is_correct = your_review_logic(eval)
    
    client.send_feedback(
        prompt_slug="your-judge-task-slug",
        completion_id=eval["span_id"],
        thumbs_up=is_correct,
        reason="Automated review",
        judge_id="your-judge-id",
    )
```

## Related

- [Pulling Evaluations](/judges/pull-evaluations) - Retrieve judge evaluations programmatically
- [Python SDK Reference](/tracing/sdks/python/reference) - Full SDK API reference
- [Judge Setup](/judges/setup) - Configure and deploy judges
